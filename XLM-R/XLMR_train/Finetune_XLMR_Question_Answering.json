{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ViDlOjnc7WZD"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-JqsjB1wk7he"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbBxgk6Owkyj"},"outputs":[],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6y3vN43A7g40"},"outputs":[],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tegH1wwPNT_1"},"outputs":[],"source":["!git clone https://github.com/manhtientran/transformers-v1.git\n","!pip install ./transformers-v1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCQ8sA3Ljjyp"},"outputs":[],"source":["\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/train.json\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/dev.json\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/test.json\n","\n","\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/vi_squad-translate-train-train-v1.1.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gT_5NUAXDSCk"},"outputs":[],"source":["import os\n","import random\n","import timeit\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","\n","import transformers\n","from transformers import (\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModelForQuestionAnswering,\n","    AutoTokenizer,\n","    get_linear_schedule_with_warmup,\n","    squad_convert_examples_to_features,\n",")\n","from transformers.data.metrics.squad_metrics import (\n","    compute_predictions_log_probs,\n","    compute_predictions_logits,\n","    squad_evaluate,\n",")\n","from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n","from transformers.trainer_utils import is_main_process\n","\n","try:\n","    from torch.utils.tensorboard import SummaryWriter\n","except ImportError:\n","    from tensorboardX import SummaryWriter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LapIK-geeLN0"},"outputs":[],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85mxdW1TeXQ7"},"outputs":[],"source":["!lsb_release -a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3eSncUmIee-q"},"outputs":[],"source":["torch.version.cuda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7sn6vpCheZgN"},"outputs":[],"source":["# !dpkg -l | grep Nvidia\n","# !sudo apt-get purge nvidia*\n","\n","# # Colab Ubuntu 18.04 CUDA 11.3\n","# # https://developer.nvidia.com/cuda-11-3-1-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=18.04&target_type=runfile_local\n","# !wget https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda_11.3.1_465.19.01_linux.run\n","# !sudo sh cuda_11.3.1_465.19.01_linux.run --toolkit --silent --override"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3z34eR_pekXj"},"outputs":[],"source":["# # INSTALL APEX\n","\n","# !git clone https://github.com/NVIDIA/apex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Th5ABIGenNU"},"outputs":[],"source":["# cd apex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mj4vUrajeqCj"},"outputs":[],"source":["# # Colab\n","# !CUDA_HOME=/usr/local/cuda-11.3 pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ML0d2NF0evU0"},"outputs":[],"source":["# cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmgUKpQse2g8"},"outputs":[],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDpAZzFtDZ0C"},"outputs":[],"source":["device = torch.device(\"cuda\")\n","n_gpu = torch.cuda.device_count()\n","print(\"device \", device)\n","print(\"n_gpu \", n_gpu)"]},{"cell_type":"markdown","metadata":{"id":"4Bg-nIoUFi37"},"source":["overwrite_cache = True\n","\n","do_lower_case = False\n","\n","gradient_accumulation_steps = 1\n","\n","max_grad_norm = 1.0\n","\n","verbose_logging = True (to log related message when processing data)\n","\n","overwrite_output_dir = True\n","\n","local_rank = -1\n","\n","version_2_with_negative = False\n","\n","threads = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0YQOTgnDpfq"},"outputs":[],"source":["CONFIG = {\n","    \"model_name_or_path\": \"/content/drive/MyDrive/IT4998/models/XLMR-Question-Answering-July_10_2022_02h_57m_21s\",\n","    \"n_gpu\" : n_gpu,\n","    \"device\": device,\n","    \"per_gpu_train_batch_size\" : 3,\n","    \"per_gpu_eval_batch_size\": 3,\n","    \"num_train_epochs\": 2,          # DEFAULT 3\n","    \"weight_decay\": 0.01,\n","    \"learning_rate\": 3e-5,\n","    \"adam_epsilon\": 1e-8,\n","    \"warmup_steps\": 365,\n","    \"fp16\": False,           # default\n","    \"fp16_opt_level\": \"O1\",  # default\n","    \"seed\": 42,\n","    \"logging_steps\": 5000,\n","    \"evaluate_during_training\": True,\n","    \"data_dir\": \".\",\n","    \"max_seq_length\": 512,\n","    \"predict_file\": \"test.json\",\n","    \"train_file\": \"train.json\",\n","    \"doc_stride\": 256,      # DEFAULT 128\n","    \"output_dir\": \"/content/drive/MyDrive/IT4998/models\",\n","    \"n_best_size\": 3,\n","    \"max_query_length\": 130,\n","    \"max_answer_length\": 370,\n","    \"do_train\": False,\n","    \"do_eval\": True,\n","    \"verbose_logging\": False\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4L5Og65wEEFj"},"outputs":[],"source":["for key, value in CONFIG.items():\n","    print(key, ' : ', value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8hhTI3DEJoa"},"outputs":[],"source":["from datetime import datetime\n","\n","unique_model_name = \"XLMR-Question-Answering-{time}\".format(\n","    time=datetime.now().strftime(\"%B_%d_%Y_%Hh_%Mm_%Ss\"))\n","\n","CONFIG[\"output_dir\"] = os.path.join(CONFIG[\"output_dir\"], unique_model_name)\n","print(CONFIG[\"output_dir\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rp_n47AmEORS"},"outputs":[],"source":["def set_seed():\n","    random.seed(CONFIG[\"seed\"])\n","    np.random.seed(CONFIG[\"seed\"])\n","    torch.manual_seed(CONFIG[\"seed\"])\n","    if CONFIG[\"n_gpu\"] > 0:\n","        torch.cuda.manual_seed_all(CONFIG[\"seed\"])\n","\n","def to_list(tensor):\n","    return tensor.detach().cpu().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsfKcQseEj6N"},"outputs":[],"source":["def evaluate(model, tokenizer, prefix=\"\"):\n","    dataset, examples, features = load_and_cache_examples(tokenizer, evaluate=True, output_examples=True)\n","\n","    if not os.path.exists(CONFIG[\"output_dir\"]):\n","        os.makedirs(CONFIG[\"output_dir\"])\n","\n","    CONFIG[\"eval_batch_size\"] = CONFIG[\"per_gpu_eval_batch_size\"] * max(1, CONFIG[\"n_gpu\"])\n","\n","    # Note that DistributedSampler samples randomly\n","    eval_sampler = SequentialSampler(dataset)\n","    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=CONFIG[\"eval_batch_size\"])\n","\n","    # multi-gpu evaluate\n","    if CONFIG[\"n_gpu\"] > 1 and not isinstance(model, torch.nn.DataParallel):\n","        model = torch.nn.DataParallel(model)\n","\n","    # Eval!\n","    print(\"***** Running evaluation {} *****\".format(prefix))\n","    print(\"  Num examples = %d\", len(dataset))\n","    print(\"  Batch size = %d\", CONFIG[\"eval_batch_size\"])\n","\n","    all_results = []\n","    start_time = timeit.default_timer()\n","\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(CONFIG[\"device\"]) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","            }\n","\n","            # del inputs[\"token_type_ids\"]\n","\n","            feature_indices = batch[3]\n","\n","            outputs = model(**inputs)\n","\n","        for i, feature_index in enumerate(feature_indices):\n","            eval_feature = features[feature_index.item()]\n","            unique_id = int(eval_feature.unique_id)\n","\n","            output = [to_list(output[i]) for output in outputs.to_tuple()]\n","\n","            # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n","            # models only use two.\n","            if len(output) >= 5:\n","                start_logits = output[0]\n","                start_top_index = output[1]\n","                end_logits = output[2]\n","                end_top_index = output[3]\n","                cls_logits = output[4]\n","\n","                result = SquadResult(\n","                    unique_id,\n","                    start_logits,\n","                    end_logits,\n","                    start_top_index=start_top_index,\n","                    end_top_index=end_top_index,\n","                    cls_logits=cls_logits,\n","                )\n","\n","            else:\n","                start_logits, end_logits = output\n","                result = SquadResult(unique_id, start_logits, end_logits)\n","\n","            all_results.append(result)\n","\n","    evalTime = timeit.default_timer() - start_time\n","    # print(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n","\n","    # Compute predictions\n","    output_prediction_file = os.path.join(CONFIG[\"output_dir\"], \"predictions_{}.json\".format(prefix))\n","    output_nbest_file = os.path.join(CONFIG[\"output_dir\"], \"nbest_predictions_{}.json\".format(prefix))\n","\n","    output_null_log_odds_file = None    # because version_2_with_negative = False\n","\n","    predictions = compute_predictions_logits(\n","        examples,\n","        features,\n","        all_results,\n","        CONFIG[\"n_best_size\"],\n","        CONFIG[\"max_answer_length\"],\n","        False,\n","        output_prediction_file,\n","        output_nbest_file,\n","        output_null_log_odds_file,\n","        CONFIG[\"verbose_logging\"],       # verbose logging\n","        False,\n","        0.0,\n","        tokenizer,\n","    )\n","\n","    # Compute the F1 and exact scores.\n","    results = squad_evaluate(examples, predictions)\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjP8CaJ-FSw8"},"outputs":[],"source":["def load_and_cache_examples(tokenizer, evaluate=False, output_examples=False):\n","    # Load data features from cache or dataset file\n","    input_dir = CONFIG[\"data_dir\"] if CONFIG[\"data_dir\"] else \".\"\n","    \n","    # cached_features_file = os.path.join(\n","    #     input_dir,\n","    #     \"cached_{}_{}_{}\".format(\n","    #         \"dev\" if evaluate else \"train\",\n","    #         list(filter(None, CONFIG[\"model_name_or_path\"].split(\"/\"))).pop(),\n","    #         str(CONFIG[\"max_seq_length\"]),\n","    #     ),\n","    # )\n","\n","    # print(\"Creating features from dataset file at {}\".format(input_dir))\n","\n","    processor = SquadV1Processor()\n","    if evaluate:\n","        examples = processor.get_dev_examples(CONFIG[\"data_dir\"], filename=CONFIG[\"predict_file\"])\n","    else:\n","        examples = processor.get_train_examples(CONFIG[\"data_dir\"], filename=CONFIG[\"train_file\"])\n","\n","    features, dataset = squad_convert_examples_to_features(\n","        examples=examples,\n","        tokenizer=tokenizer,\n","        max_seq_length=CONFIG[\"max_seq_length\"],\n","        doc_stride=CONFIG[\"doc_stride\"],\n","        max_query_length=CONFIG[\"max_query_length\"],\n","        is_training=not evaluate,\n","        return_dataset=\"pt\",\n","        threads=1,\n","    )\n","\n","    # print(\"Saving features into cached file %s\", cached_features_file)\n","    # torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n","\n","    if output_examples:\n","        return dataset, examples, features\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xp-5rmswHfZr"},"outputs":[],"source":["if CONFIG[\"doc_stride\"] >= CONFIG[\"max_seq_length\"] - CONFIG[\"max_query_length\"]:\n","    print(\"\"\"\n","        WARNING - You've set a doc stride which may be superior to the document length in some \n","        examples. This could result in errors when building features from the examples. Please reduce the doc \n","        stride or increase the maximum length to ensure the features are correctly built.\"\n","        \"\"\"\n","    )\n","\n","if is_main_process(-1):\n","    transformers.utils.logging.set_verbosity_info()\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","    \n","set_seed()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jaVti3CHvVc"},"outputs":[],"source":["# Load pretrained model and tokenizer\n","config = AutoConfig.from_pretrained(CONFIG[\"model_name_or_path\"])\n","tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name_or_path\"], do_lower_case = False, use_fast=False)\n","model = AutoModelForQuestionAnswering.from_pretrained(\n","    CONFIG[\"model_name_or_path\"],\n","    from_tf=bool(\".ckpt\" in CONFIG[\"model_name_or_path\"]),\n","    config=config\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dq_sStsJHy_S"},"outputs":[],"source":["model.to(CONFIG[\"device\"])\n","\n","if CONFIG[\"fp16\"]:\n","    try:\n","        import apex\n","\n","        apex.amp.register_half_function(torch, \"einsum\")\n","    except ImportError:\n","        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtPSZGS3H7K5"},"outputs":[],"source":["# Training\n","if CONFIG[\"do_train\"]:\n","    train_dataset = load_and_cache_examples(tokenizer, evaluate=False, output_examples=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYOZDPSjXUfF"},"outputs":[],"source":["if CONFIG[\"do_train\"]:\n","    tb_writer = SummaryWriter()\n","\n","    CONFIG[\"train_batch_size\"] = CONFIG[\"per_gpu_train_batch_size\"] * max(1, CONFIG[\"n_gpu\"])\n","    train_sampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=CONFIG[\"train_batch_size\"])\n","\n","    t_total = len(train_dataloader) // 1 * CONFIG[\"num_train_epochs\"]\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": CONFIG[\"weight_decay\"],\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=CONFIG[\"learning_rate\"], eps=CONFIG[\"adam_epsilon\"])\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=CONFIG[\"warmup_steps\"], num_training_steps=t_total\n","    )\n","\n","    if CONFIG[\"fp16\"]:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=CONFIG[\"fp16_opt_level\"])\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if CONFIG[\"n_gpu\"] > 1:\n","        model = torch.nn.DataParallel(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxLy-5n24rzL"},"outputs":[],"source":["EVAL_EACH_EPOCH = \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJzxuOsOYB8V"},"outputs":[],"source":["if CONFIG[\"do_train\"]:\n","    # Train!\n","    print(\"***** Running training *****\")\n","    print(\"  Num examples = %d\", len(train_dataset))\n","    print(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 1\n","    epochs_trained = 0\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    train_iterator = trange(\n","        epochs_trained, int(CONFIG[\"num_train_epochs\"]), desc=\"Epoch\", disable=(-1 not in [-1, 0])\n","    )\n","    # Added here for reproductibility\n","    set_seed()\n","\n","    from functools import partial\n","    tqdm = partial(tqdm, position=0, leave=True)\n","\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=(-1 not in [-1, 0]))\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            batch = tuple(t.to(CONFIG[\"device\"]) for t in batch)\n","\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","                \"start_positions\": batch[3],\n","                \"end_positions\": batch[4],\n","            }\n","\n","            # del inputs[\"token_type_ids\"]\n","\n","            outputs = model(**inputs)\n","            # model outputs are always tuple in transformers (see doc)\n","            loss = outputs[0]\n","\n","            if CONFIG[\"n_gpu\"] > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n","\n","            if CONFIG[\"fp16\"]:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","\n","            if CONFIG[\"fp16\"]:\n","                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 1.0)\n","            else:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()  # Update learning rate schedule\n","            model.zero_grad()\n","            global_step += 1\n","\n","        # Log metrics\n","        # Only evaluate when single GPU otherwise metrics may not average well\n","        if CONFIG[\"evaluate_during_training\"]:\n","            results = evaluate(model, tokenizer)\n","            print(\"Dev Set - Exact Match: \" + str(results['exact']))\n","            print(\"Dev Set - F1-score: \" + str(results['f1']))\n","\n","            EVAL_EACH_EPOCH += \"Epoch \" + str(_.real) + \": \\n\"\n","            EVAL_EACH_EPOCH += \"Dev Set - Exact Match: \" + str(results['exact']) + \"\\n\"\n","            EVAL_EACH_EPOCH += \"Dev Set - F1-score: \" + str(results['f1']) + \"\\n\"\n","            EVAL_EACH_EPOCH += \"\\n\"\n","            \n","        print(\"Learning rate: \" + str(scheduler.get_lr()[0]))\n","\n","        # tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / CONFIG[\"logging_steps\"], global_step)\n","        logging_loss = tr_loss\n","\n","    tb_writer.close()\n","\n","    print(\"global_step = %s, average loss = %s\", global_step, tr_loss)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNLXz_tq5jnE"},"outputs":[],"source":["if CONFIG[\"do_train\"]:\n","    with open(os.path.join(CONFIG[\"output_dir\"], \"each_epoch_info_\" + unique_model_name +\".txt\"), \"w\") as f:\n","        f.write(EVAL_EACH_EPOCH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-f-vRhgY9TV"},"outputs":[],"source":["# Save the trained model and the tokenizer\n","if CONFIG[\"do_train\"]:\n","    print(\"Saving model checkpoint to %s\", CONFIG[\"output_dir\"])\n","\n","    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","    # They can then be reloaded using `from_pretrained()`\n","    # Take care of distributed/parallel training\n","    model_to_save = model.module if hasattr(model, \"module\") else model\n","    model_to_save.save_pretrained(CONFIG[\"output_dir\"])\n","    tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n","\n","    # Good practice: save your training arguments together with the trained model\n","    # torch.save(CONFIG, os.path.join(CONFIG[\"output_dir\"], \"training_args.txt\"))\n","    with open(os.path.join(CONFIG[\"output_dir\"], \"config_\" + unique_model_name +\".txt\"), \"w\") as f:\n","        res = \"\"\n","        for key, value in CONFIG.items():\n","            res += str(key) + \" : \" + str(value) +  \"\\n\"\n","        f.write(res)\n","\n","    # Load a trained model and vocabulary that you have fine-tuned\n","    model = AutoModelForQuestionAnswering.from_pretrained(CONFIG[\"output_dir\"])  # , force_download=True)\n","\n","    # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling\n","    # So we use use_fast=False here for now until Fast-tokenizer-compatible-examples are out\n","    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"output_dir\"], do_lower_case=False, use_fast=False)\n","    model.to(CONFIG[\"device\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsWtqP1JZQpj"},"outputs":[],"source":["# Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n","results = {}\n","if CONFIG[\"do_eval\"]:\n","    if CONFIG[\"do_train\"]:\n","        print(\"Loading checkpoints saved during training for evaluation\")\n","        checkpoints = [CONFIG[\"output_dir\"]]\n","\n","    else:\n","        print(\"Loading checkpoint %s for evaluation\", CONFIG[\"model_name_or_path\"])\n","        checkpoints = [CONFIG[\"model_name_or_path\"]]\n","\n","    print(\"Evaluate the following checkpoints: %s\", checkpoints)\n","\n","    for checkpoint in checkpoints:\n","        # Reload the model\n","        global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","        model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)\n","        model.to(CONFIG[\"device\"])\n","\n","        # Evaluate\n","        result = evaluate(model, tokenizer, prefix=global_step)\n","\n","        result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n","        results.update(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3e9Ik7uZq49"},"outputs":[],"source":["for key, value in results.items():\n","    print(key, \":\", value)\n","\n","if CONFIG[\"do_train\"]:\n","    with open(os.path.join(CONFIG[\"output_dir\"], \"result_\" + unique_model_name +\".txt\"), \"w\") as f:\n","        res = \"\"\n","        for key, value in results.items():\n","            res += str(key) + \" : \" + str(value) +  \"\\n\"\n","        f.write(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9F_pAYcnxIAD"},"outputs":[],"source":["if CONFIG[\"do_train\"]:\n","    with open(os.path.join(CONFIG[\"output_dir\"], \"machine_info_\" + unique_model_name +\".txt\"), \"w\") as f:\n","        f.write(\"GPU INFO:\\n\")\n","        f.write(gpu_info)\n","        f.write(\"\\n\")\n","        f.write(\"RAM INFO:\\n\")\n","        f.write('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Finetune_XLMR_Question_Answering","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}