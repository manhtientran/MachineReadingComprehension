{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluate_Language_model_for_ViQuAD.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WpW4WECXRYo5"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"id":"zcLycYv8h5XM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"7-f-rf2qry8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"kNjep4TBhTaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/requirements.txt\n","\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/train.json\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/dev.json\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/test.json\n","\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/Data_for_training_MLM/viquad_mlm_train.txt\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/Data_for_training_MLM/viquad_mlm_dev.txt\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/Data_for_training_MLM/viquad_mlm_test.txt\n","\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/vi_squad-translate-train-train-v1.1.json\n","\n","\n","!pip install -r requirements.txt"],"metadata":{"id":"Pm5jwVZURfWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import math\n","import os\n","import random\n","from itertools import chain\n","from pathlib import Path\n","\n","import datasets\n","import torch\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","\n","import transformers\n","from accelerate import Accelerator, DistributedType\n","from accelerate.logging import get_logger\n","from accelerate.utils import set_seed\n","from huggingface_hub import Repository\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_MAPPING,\n","    AutoConfig,\n","    AutoModelForMaskedLM,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    SchedulerType,\n","    get_scheduler,\n",")\n","from transformers.utils import get_full_repo_name, send_example_telemetry\n","from transformers.utils.versions import require_version"],"metadata":{"id":"rZwwsvmlRi0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")"],"metadata":{"id":"pW7qspLeRkjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["with_tracking = False\n","\n","preprocessing_num_workers = None"],"metadata":{"id":"yYqlRmzahjag"}},{"cell_type":"code","source":["# Default Config\n","\n","CONFIG = {\n","    \"train_file\": \"viquad_mlm_train.txt\",\n","    \"validation_file\": \"viquad_mlm_test.txt\",\n","    \"pad_to_max_length\": True,\n","    \"model_name_or_path\": \"/content/drive/MyDrive/IT4998/models/PhoBERT-Language-Modeling-June_26_2022_02h_46m_53s\",\n","    \"use_slow_tokenizer\": True,\n","    \"per_device_train_batch_size\": 68,\n","    \"per_device_eval_batch_size\": 68,\n","    \"learning_rate\": 4e-5,\n","    \"weight_decay\": 0.01,\n","    \"num_train_epochs\": 3,          # DEFAULT 3\n","    \"gradient_accumulation_steps\": 1,\n","    \"lr_scheduler_type\": \"linear\",\n","    \"num_warmup_steps\": 0,\n","    \"output_dir\": \"/content/drive/MyDrive/IT4998/models\",\n","    \"seed\": 42,\n","    \"max_seq_length\": 256,\n","    \"line_by_line\": True,\n","    \"overwrite_cache\": True,\n","    \"mlm_probability\": 0.15\n","}\n","\n","for key, value in CONFIG.items():\n","    print(key, \":\", value)"],"metadata":{"id":"9W2GwotLRl83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accelerator = Accelerator()\n","\n","if accelerator.is_local_main_process:\n","    datasets.utils.logging.set_verbosity_warning()\n","    transformers.utils.logging.set_verbosity_info()\n","else:\n","    datasets.utils.logging.set_verbosity_error()\n","    transformers.utils.logging.set_verbosity_error()\n","\n","set_seed(CONFIG[\"seed\"])\n","\n","if accelerator.is_main_process:\n","    if CONFIG[\"output_dir\"] is not None:\n","        os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n","\n","accelerator.wait_for_everyone()"],"metadata":{"id":"AmnocgsbRrA3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n","# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n","# (the dataset will be downloaded automatically from the datasets Hub).\n","#\n","# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n","# 'text' is found. You can easily tweak this behavior (see below).\n","#\n","# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n","# download the dataset.\n","\n","data_files = {}\n","if CONFIG[\"train_file\"] is not None:\n","    data_files[\"train\"] = CONFIG[\"train_file\"]\n","if CONFIG[\"validation_file\"] is not None:\n","    data_files[\"validation\"] = CONFIG[\"validation_file\"]\n","extension = CONFIG[\"train_file\"].split(\".\")[-1]\n","if extension == \"txt\":\n","    extension = \"text\"\n","raw_datasets = load_dataset(extension, data_files=data_files)"],"metadata":{"id":"dlj1EkWCRtjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pretrained model and tokenizer\n","#\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","\n","config = AutoConfig.from_pretrained(CONFIG[\"model_name_or_path\"])\n","tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name_or_path\"], use_fast=not CONFIG[\"use_slow_tokenizer\"])\n","model = AutoModelForMaskedLM.from_pretrained(\n","            CONFIG[\"model_name_or_path\"],\n","            from_tf=bool(\".ckpt\" in CONFIG[\"model_name_or_path\"]),\n","            config=config,\n","        )\n","\n","model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"byOtNFCqRx5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing the datasets.\n","# First we tokenize all the texts.\n","column_names = raw_datasets[\"train\"].column_names\n","text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n","\n","if CONFIG[\"max_seq_length\"] > tokenizer.model_max_length:\n","    print(\n","        \"\"\"The max_seq_length passed {} is larger than the maximum length for the\n","        model {}. Using max_seq_length={}.\"\"\".format(CONFIG[\"max_seq_length\"], tokenizer.model_max_length, tokenizer.model_max_length)\n","    )\n","\n","max_seq_length = min(CONFIG[\"max_seq_length\"], tokenizer.model_max_length)\n","print(\"max_seq_length :\", max_seq_length)"],"metadata":{"id":"wQ3uXA2Dh6tf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if CONFIG[\"line_by_line\"]:\n","    # When using line_by_line, we just tokenize each nonempty line.\n","    padding = \"max_length\" if CONFIG[\"pad_to_max_length\"] else False\n","\n","    def tokenize_function(examples):\n","        # Remove empty lines\n","        examples[text_column_name] = [\n","            line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()\n","        ]\n","        return tokenizer(\n","            examples[text_column_name],\n","            padding=padding,\n","            truncation=True,\n","            max_length=max_seq_length,\n","            # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n","            # receives the `special_tokens_mask`.\n","            return_special_tokens_mask=True,\n","        )\n","\n","    with accelerator.main_process_first():\n","        tokenized_datasets = raw_datasets.map(\n","            tokenize_function,\n","            batched=True,\n","            num_proc=None,\n","            remove_columns=[text_column_name],\n","            load_from_cache_file=not CONFIG[\"overwrite_cache\"],\n","            desc=\"Running tokenizer on dataset line_by_line\",\n","        )\n","else:\n","    # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n","    # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n","    # efficient when it receives the `special_tokens_mask`.\n","    def tokenize_function(examples):\n","        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n","\n","    with accelerator.main_process_first():\n","        tokenized_datasets = raw_datasets.map(\n","            tokenize_function,\n","            batched=True,\n","            num_proc=None,\n","            remove_columns=column_names,\n","            load_from_cache_file=not CONFIG[\"overwrite_cache\"],\n","            desc=\"Running tokenizer on every text in dataset\",\n","        )\n","\n","    # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n","    # max_seq_length.\n","    def group_texts(examples):\n","        # Concatenate all texts.\n","        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n","        total_length = len(concatenated_examples[list(examples.keys())[0]])\n","        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","        # customize this part to your needs.\n","        if total_length >= max_seq_length:\n","            total_length = (total_length // max_seq_length) * max_seq_length\n","        # Split by chunks of max_len.\n","        result = {\n","            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n","            for k, t in concatenated_examples.items()\n","        }\n","        return result\n","\n","    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n","    # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n","    # might be slower to preprocess.\n","    #\n","    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n","    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n","\n","    with accelerator.main_process_first():\n","        tokenized_datasets = tokenized_datasets.map(\n","            group_texts,\n","            batched=True,\n","            num_proc=None,\n","            load_from_cache_file=not CONFIG[\"overwrite_cache\"],\n","            desc=f\"Grouping texts in chunks of {max_seq_length}\",\n","        )"],"metadata":{"id":"ga-hqMzYR1BX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_dataset = tokenized_datasets[\"validation\"]"],"metadata":{"id":"yuXNrK2FSDhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=CONFIG[\"mlm_probability\"])\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=CONFIG[\"per_device_eval_batch_size\"])"],"metadata":{"id":"p8J3WHHmSGKP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n","if accelerator.distributed_type == DistributedType.TPU:\n","    model.tie_weights()"],"metadata":{"id":"S-kDO_igSK_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model, eval_dataloader = accelerator.prepare(model, eval_dataloader)"],"metadata":{"id":"Pw4Jyf5_SR6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","losses = []\n","for step, batch in enumerate(eval_dataloader):\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    loss = outputs.loss\n","    losses.append(accelerator.gather(loss.repeat(CONFIG[\"per_device_eval_batch_size\"])))\n","\n","losses = torch.cat(losses)\n","losses = losses[: len(eval_dataset)]\n","print(losses)\n","\n","try:\n","    eval_loss = torch.mean(losses)\n","    print(\"eval_loss: \", eval_loss)\n","    perplexity = math.exp(eval_loss)\n","except OverflowError:\n","    perplexity = float(\"inf\")\n","\n","print(\"Dev Set Perplexity: {perplexity}\".format(perplexity=perplexity))"],"metadata":{"id":"N0YuB7MVSmJ3"},"execution_count":null,"outputs":[]}]}