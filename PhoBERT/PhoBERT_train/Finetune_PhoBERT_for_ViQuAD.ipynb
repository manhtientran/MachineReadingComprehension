{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Finetune_PhoBERT_for_ViQuAD.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sPEfv7vY6sIV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"id":"XvJMRMZyjCU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"UP7xVQRGmAWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"vd1Br0C6onN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/requirements.txt\n","\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/train.json\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/dev.json\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/test.json\n","\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/Data_for_training_MLM/viquad_mlm_train.txt\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/Data_for_training_MLM/viquad_mlm_dev.txt\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/Data_for_training_MLM/viquad_mlm_test.txt\n","\n","!curl -O https://raw.githubusercontent.com/manhtientran/4998-Scripts/master/vi_squad-translate-train-train-v1.1.json\n","\n","\n","!pip install -r requirements.txt"],"metadata":{"id":"rgTNBrVb7xnt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import math\n","import os\n","import random\n","from itertools import chain\n","from pathlib import Path\n","\n","import datasets\n","import torch\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","\n","import transformers\n","from accelerate import Accelerator, DistributedType\n","from accelerate.logging import get_logger\n","from accelerate.utils import set_seed\n","from huggingface_hub import Repository\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_MAPPING,\n","    AutoConfig,\n","    AutoModelForMaskedLM,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    SchedulerType,\n","    get_scheduler,\n",")\n","from transformers.utils import get_full_repo_name\n","from transformers.utils.versions import require_version"],"metadata":{"id":"BM5cyhGb7TrX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")"],"metadata":{"id":"3oGEjFg17wMV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["with_tracking = False\n","\n","preprocessing_num_workers = None"],"metadata":{"id":"yYqlRmzahjag"}},{"cell_type":"code","source":["# Default Config\n","\n","CONFIG = {\n","    \"train_file\": \"viquad_mlm_train.txt\",\n","    \"validation_file\": \"viquad_mlm_dev.txt\",\n","    \"pad_to_max_length\": True,\n","    \"model_name_or_path\": \"vinai/phobert-large\",\n","    \"use_slow_tokenizer\": True,\n","    \"per_device_train_batch_size\": 9,\n","    \"per_device_eval_batch_size\": 9,\n","    \"learning_rate\": 3.9e-5,\n","    \"weight_decay\": 0.01,\n","    \"num_train_epochs\": 3,          # DEFAULT 3\n","    \"gradient_accumulation_steps\": 1,\n","    \"lr_scheduler_type\": \"linear\",\n","    \"num_warmup_steps\": 0,\n","    \"output_dir\": \"/content/drive/MyDrive/IT4998/models\",\n","    \"seed\": 42,\n","    \"max_seq_length\": 256,\n","    \"line_by_line\": True,\n","    \"overwrite_cache\": True,\n","    \"mlm_probability\": 0.15\n","}\n","\n","for key, value in CONFIG.items():\n","    print(key, \":\", value)"],"metadata":{"id":"AswzMF0i7wqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","unique_model_name = \"PhoBERT-Language-Modeling-{time}\".format(\n","    time=datetime.now().strftime(\"%B_%d_%Y_%Hh_%Mm_%Ss\"))\n","\n","CONFIG[\"output_dir\"] = os.path.join(CONFIG[\"output_dir\"], unique_model_name)\n","print(CONFIG[\"output_dir\"])"],"metadata":{"id":"hZxZ6esiyCWW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accelerator = Accelerator()\n","\n","if accelerator.is_local_main_process:\n","    datasets.utils.logging.set_verbosity_warning()\n","    transformers.utils.logging.set_verbosity_info()\n","else:\n","    datasets.utils.logging.set_verbosity_error()\n","    transformers.utils.logging.set_verbosity_error()\n","\n","set_seed(CONFIG[\"seed\"])\n","\n","if accelerator.is_main_process:\n","    if CONFIG[\"output_dir\"] is not None:\n","        os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n","\n","accelerator.wait_for_everyone()"],"metadata":{"id":"Pqe39tgyhmpv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n","# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n","# (the dataset will be downloaded automatically from the datasets Hub).\n","#\n","# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n","# 'text' is found. You can easily tweak this behavior (see below).\n","#\n","# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n","# download the dataset.\n","\n","data_files = {}\n","if CONFIG[\"train_file\"] is not None:\n","    data_files[\"train\"] = CONFIG[\"train_file\"]\n","if CONFIG[\"validation_file\"] is not None:\n","    data_files[\"validation\"] = CONFIG[\"validation_file\"]\n","extension = CONFIG[\"train_file\"].split(\".\")[-1]\n","if extension == \"txt\":\n","    extension = \"text\"\n","raw_datasets = load_dataset(extension, data_files=data_files)"],"metadata":{"id":"-i9Ssv6kiXtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pretrained model and tokenizer\n","#\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","\n","config = AutoConfig.from_pretrained(CONFIG[\"model_name_or_path\"])\n","tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name_or_path\"], use_fast=not CONFIG[\"use_slow_tokenizer\"])\n","model = AutoModelForMaskedLM.from_pretrained(\n","            CONFIG[\"model_name_or_path\"],\n","            from_tf=bool(\".ckpt\" in CONFIG[\"model_name_or_path\"]),\n","            config=config,\n","        )\n","\n","model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"TLiYi0H1jCc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing the datasets.\n","# First we tokenize all the texts.\n","column_names = raw_datasets[\"train\"].column_names\n","text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n","\n","if CONFIG[\"max_seq_length\"] > tokenizer.model_max_length:\n","    print(\n","        \"\"\"The max_seq_length passed {} is larger than the maximum length for the\n","        model {}. Using max_seq_length={}.\"\"\".format(CONFIG[\"max_seq_length\"], tokenizer.model_max_length, tokenizer.model_max_length)\n","    )\n","\n","max_seq_length = min(CONFIG[\"max_seq_length\"], tokenizer.model_max_length)\n","print(\"max_seq_length :\", max_seq_length)"],"metadata":{"id":"ud0YRUygjYmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if CONFIG[\"line_by_line\"]:\n","    # When using line_by_line, we just tokenize each nonempty line.\n","    padding = \"max_length\" if CONFIG[\"pad_to_max_length\"] else False\n","\n","    def tokenize_function(examples):\n","        # Remove empty lines\n","        examples[text_column_name] = [\n","            line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()\n","        ]\n","        return tokenizer(\n","            examples[text_column_name],\n","            padding=padding,\n","            truncation=True,\n","            max_length=max_seq_length,\n","            # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n","            # receives the `special_tokens_mask`.\n","            return_special_tokens_mask=True,\n","        )\n","\n","    with accelerator.main_process_first():\n","        tokenized_datasets = raw_datasets.map(\n","            tokenize_function,\n","            batched=True,\n","            num_proc=None,\n","            remove_columns=[text_column_name],\n","            load_from_cache_file=not CONFIG[\"overwrite_cache\"],\n","            desc=\"Running tokenizer on dataset line_by_line\",\n","        )\n","else:\n","    # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n","    # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n","    # efficient when it receives the `special_tokens_mask`.\n","    def tokenize_function(examples):\n","        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n","\n","    with accelerator.main_process_first():\n","        tokenized_datasets = raw_datasets.map(\n","            tokenize_function,\n","            batched=True,\n","            num_proc=None,\n","            remove_columns=column_names,\n","            load_from_cache_file=not CONFIG[\"overwrite_cache\"],\n","            desc=\"Running tokenizer on every text in dataset\",\n","        )\n","\n","    # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n","    # max_seq_length.\n","    def group_texts(examples):\n","        # Concatenate all texts.\n","        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n","        total_length = len(concatenated_examples[list(examples.keys())[0]])\n","        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","        # customize this part to your needs.\n","        if total_length >= max_seq_length:\n","            total_length = (total_length // max_seq_length) * max_seq_length\n","        # Split by chunks of max_len.\n","        result = {\n","            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n","            for k, t in concatenated_examples.items()\n","        }\n","        return result\n","\n","    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n","    # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n","    # might be slower to preprocess.\n","    #\n","    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n","    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n","\n","    with accelerator.main_process_first():\n","        tokenized_datasets = tokenized_datasets.map(\n","            group_texts,\n","            batched=True,\n","            num_proc=None,\n","            load_from_cache_file=not CONFIG[\"overwrite_cache\"],\n","            desc=f\"Grouping texts in chunks of {max_seq_length}\",\n","        )"],"metadata":{"id":"yLRChYWrj_Qf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = tokenized_datasets[\"train\"]\n","eval_dataset = tokenized_datasets[\"validation\"]\n","\n","# Conditional for small test subsets\n","if len(train_dataset) > 3:\n","    # Log a few random samples from the training set:\n","    for index in random.sample(range(len(train_dataset)), 3):\n","        print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"],"metadata":{"id":"wHLL9aNnkd1Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data collator\n","# This one will take care of randomly masking the tokens.\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=CONFIG[\"mlm_probability\"])\n","\n","# DataLoaders creation:\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=CONFIG[\"per_device_train_batch_size\"]\n",")\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=CONFIG[\"per_device_eval_batch_size\"])\n","\n","# Optimizer\n","# Split weights in two groups, one with weight decay and the other not.\n","no_decay = [\"bias\", \"LayerNorm.weight\"]\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","        \"weight_decay\": CONFIG[\"weight_decay\"],\n","    },\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=CONFIG[\"learning_rate\"])\n"],"metadata":{"id":"Ivz9exdPkgcf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n","if accelerator.distributed_type == DistributedType.TPU:\n","    model.tie_weights()"],"metadata":{"id":"eju5U6E9k-iW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n","# shorter in multiprocess)\n","\n","# Scheduler and math around the number of training steps.\n","overrode_max_train_steps = False\n","\n","num_update_steps_per_epoch = math.ceil(len(train_dataloader) / CONFIG[\"gradient_accumulation_steps\"])\n","CONFIG[\"max_train_steps\"] = CONFIG[\"num_train_epochs\"] * num_update_steps_per_epoch\n","\n","CONFIG[\"max_train_steps\"] = CONFIG[\"num_train_epochs\"] * num_update_steps_per_epoch\n","overrode_max_train_steps = True\n","\n","lr_scheduler = get_scheduler(\n","    name=CONFIG[\"lr_scheduler_type\"],\n","    optimizer=optimizer,\n","    num_warmup_steps=CONFIG[\"num_warmup_steps\"],\n","    num_training_steps=CONFIG[\"max_train_steps\"],\n",")\n","\n","# Prepare everything with our `accelerator`.\n","model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",")\n","\n","# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","num_update_steps_per_epoch = math.ceil(len(train_dataloader) / CONFIG[\"gradient_accumulation_steps\"])\n","\n","CONFIG[\"max_train_steps\"] = CONFIG[\"num_train_epochs\"] * num_update_steps_per_epoch\n","\n","CONFIG[\"num_train_epochs\"] = math.ceil(CONFIG[\"max_train_steps\"] / num_update_steps_per_epoch)"],"metadata":{"id":"c1Fz84MJlBQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpointing_steps = None"],"metadata":{"id":"cubF4xJllsGB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train!\n","total_batch_size = CONFIG[\"per_device_train_batch_size\"] * accelerator.num_processes * CONFIG[\"gradient_accumulation_steps\"]\n","\n","print(\"***** Running training *****\")\n","print(\"  Num examples = \", len(train_dataset))\n","print(\"  Total optimization steps = \", CONFIG[\"max_train_steps\"])\n","\n","progress_bar = tqdm(range(CONFIG[\"max_train_steps\"]), disable=not accelerator.is_local_main_process)\n","completed_steps = 0\n","starting_epoch = 0\n","\n","for epoch in range(starting_epoch, CONFIG[\"num_train_epochs\"]):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        # We need to skip steps until we reach the resumed step\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        # We keep track of the loss at each epoch\n","        loss = loss / CONFIG[\"gradient_accumulation_steps\"]\n","        accelerator.backward(loss)\n","        if step % CONFIG[\"gradient_accumulation_steps\"] == 0 or step == len(train_dataloader) - 1:\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            progress_bar.update(1)\n","            completed_steps += 1\n","\n","        if isinstance(checkpointing_steps, int):\n","            if completed_steps % checkpointing_steps == 0:\n","                output_dir = f\"step_{completed_steps }\"\n","                if CONFIG[\"output_dir\"] is not None:\n","                    output_dir = os.path.join(CONFIG[\"output_dir\"], output_dir)\n","                accelerator.save_state(output_dir)\n","\n","        if completed_steps >= CONFIG[\"max_train_steps\"]:\n","            break\n","\n","    model.eval()\n","    losses = []\n","    for step, batch in enumerate(eval_dataloader):\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        loss = outputs.loss\n","        losses.append(accelerator.gather(loss.repeat(CONFIG[\"per_device_eval_batch_size\"])))\n","\n","    losses = torch.cat(losses)\n","    losses = losses[: len(eval_dataset)]\n","    try:\n","        eval_loss = torch.mean(losses)\n","        perplexity = math.exp(eval_loss)\n","    except OverflowError:\n","        perplexity = float(\"inf\")\n","\n","    print(\"Epoch {epoch}: Dev Set Perplexity: {perplexity}\".format(epoch=epoch, perplexity=perplexity))\n","    print(\"Learning rate: \" + str(lr_scheduler.get_lr()[0]))\n","\n","if CONFIG[\"output_dir\"] is not None:\n","    accelerator.wait_for_everyone()\n","    unwrapped_model = accelerator.unwrap_model(model)\n","    unwrapped_model.save_pretrained(\n","        CONFIG[\"output_dir\"], is_main_process=accelerator.is_main_process, save_function=accelerator.save\n","    )\n","    if accelerator.is_main_process:\n","        tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n","\n","    with open(os.path.join(CONFIG[\"output_dir\"], \"all_results_\" + unique_model_name + \".json\"), \"w\") as f:\n","        json.dump({\"Dev Set Perplexity\": perplexity}, f)"],"metadata":{"id":"gVHTSuRElttH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(os.path.join(CONFIG[\"output_dir\"], \"config_\" + unique_model_name +\".txt\"), \"w\") as f:\n","    res = \"\"\n","    for key, value in CONFIG.items():\n","        res += str(key) + \" : \" + str(value) +  \"\\n\"\n","    f.write(res)"],"metadata":{"id":"NmZle4SgzS-X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(os.path.join(CONFIG[\"output_dir\"], \"machine_info_\" + unique_model_name +\".txt\"), \"w\") as f:\n","  f.write(\"GPU INFO:\\n\")\n","  f.write(gpu_info)\n","  f.write(\"\\n\")\n","  f.write(\"RAM INFO:\\n\")\n","  f.write('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"],"metadata":{"id":"Shrm_79BGrRN"},"execution_count":null,"outputs":[]}]}